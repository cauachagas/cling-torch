{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"/usr/bin/wget https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.6.0%2Bcpu.zip -O libtorch.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"/usr/bin/unzip libtorch.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Atraves do LibTorch\n",
    "#pragma cling add_include_path(\"./libtorch/include\")\n",
    "#pragma cling add_include_path(\"./libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"./libtorch/lib\")\n",
    "#pragma cling load(\"libtorch\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// Atraves do PyTorch\n",
    "#pragma cling add_include_path(\"$CONDA_PREFIX/lib/python3.7/site-packages/torch/include\")\n",
    "#pragma cling add_include_path(\"$CONDA_PREFIX/lib/python3.7/site-packages/torch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"$CONDA_PREFIX/lib/python3.7/site-packages/torch/lib\")\n",
    "#pragma cling load(\"libtorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <torch/torch.h>\n",
    "#include <ATen/ATen.h>\n",
    "#include <torch/csrc/autograd/variable.h>\n",
    "#include <torch/csrc/autograd/function.h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  0  0\n",
      " 0  1  0\n",
      " 0  0  1\n",
      "[ CPUFloatType{3,3} ]\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor tensor = torch::eye(3);\n",
    "std::cout << tensor << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " 1  1\n",
      " 1  1\n",
      "[ CPUIntType{2,2} ]\n",
      "\n",
      "b: \n",
      " 0.0790  2.0317\n",
      "-0.2416  0.1248\n",
      "[ CPUFloatType{2,2} ]\n",
      "\n",
      "c: \n",
      " 1  3\n",
      " 1  1\n",
      "[ CPUIntType{2,2} ]\n"
     ]
    }
   ],
   "source": [
    "at::Tensor a = at::ones({2, 2}, at::kInt);\n",
    "at::Tensor b = at::randn({2, 2});\n",
    "auto c = a + b.to(at::kInt);\n",
    "\n",
    "std::cout << \"a: \\n\" << a << std::endl;\n",
    "std::cout << std::endl;\n",
    "std::cout << \"b: \\n\" << b << std::endl;\n",
    "std::cout << std::endl;\n",
    "std::cout << \"c: \\n\" << c << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_w: \n",
      "  1.1000\n",
      "  2.2000\n",
      "  3.3000\n",
      "  4.4000\n",
      "  5.5000\n",
      "  6.6000\n",
      "  7.7000\n",
      "  8.8000\n",
      "  9.9000\n",
      " 10.0000\n",
      "[ CPUDoubleType{10} ]\n"
     ]
    }
   ],
   "source": [
    "std::vector<double> w = {1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.0};\n",
    "auto tensor_w = at::tensor(w);\n",
    "std::cout << \"tensor_w: \\n\" << tensor_w << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "croos(a,b): \n",
      " 0.2490  0.1031 -0.4079\n",
      "-0.1204  0.4361 -0.1171\n",
      " 0.0081  0.0413 -0.0560\n",
      " 0.0434 -0.1562  0.2103\n",
      "-0.0679  0.1584 -0.5818\n",
      "-0.0461  0.0113  0.0827\n",
      " 0.7535 -0.4644 -0.2714\n",
      "-0.0148 -0.0287  0.0148\n",
      "-0.4780  0.0072  0.1448\n",
      "-0.2060  0.5731  0.0414\n",
      "[ CPUFloatType{10,3} ]\n"
     ]
    }
   ],
   "source": [
    "auto a = torch::rand({10, 3});\n",
    "auto b = torch::rand({10, 3});\n",
    "// expected\n",
    "auto exp = torch::empty({10, 3});\n",
    "for (int j = 0; j < 10; j++) {\n",
    "  auto u1 = a[j][0], u2 = a[j][1], u3 = a[j][2];\n",
    "  auto v1 = b[j][0], v2 = b[j][1], v3 = b[j][2];\n",
    "  exp[j][0] = u2 * v3 - v2 * u3;\n",
    "  exp[j][1] = v1 * u3 - u1 * v3;\n",
    "  exp[j][2] = u1 * v2 - v1 * u2;\n",
    "}\n",
    "// actual\n",
    "auto out = torch::cross(a, b);\n",
    "\n",
    "std::cout << \"croos(a,b): \\n\" << out << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd in C++ Frontend\n",
    "\n",
    "The autograd package is crucial for building highly flexible and dynamic neural networks in PyTorch. Most of the autograd APIs in PyTorch Python frontend are also available in C++ frontend, allowing easy translation of autograd code from Python to C++.\n",
    "\n",
    "In this tutorial we’ll look at several examples of doing autograd in PyTorch C++ frontend. Note that this tutorial assumes that you already have a basic understanding of autograd in Python frontend. If that’s not the case, please first read Autograd: [Automatic Differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "std::vector<c10::complex<double>> x = {\n",
    "  {1.1, -1.1}, {2.2, -2.2}, {3.3, -3.3}, {4.4, -4.4}, {5.5, -5.5},\n",
    "  {6.6, -6.6}, {7.7, -7.7}, {8.8, -8.8}, {9.9, -9.9}, {10.0, -10.0}\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic autograd operations\n",
    "\n",
    "(Adapted from [this tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#autograd-automatic-differentiation))\n",
    "\n",
    "Create a tensor and set `torch::requires_grad()` to track computation with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  1\n",
      " 1  1\n",
      "[ CPUFloatType{2,2} ]\n"
     ]
    }
   ],
   "source": [
    "auto x = torch::ones({2, 2}, torch::requires_grad());\n",
    "std::cout << x << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a tensor operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3  3\n",
      " 3  3\n",
      "[ CPUFloatType{2,2} ]\n"
     ]
    }
   ],
   "source": [
    "auto y = x + 2;\n",
    "std::cout << y << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y` was created as a result of an operation, so it has a `grad_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddBackward1\n"
     ]
    }
   ],
   "source": [
    "std::cout << y.grad_fn()->name() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do more operations on `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27  27\n",
      " 27  27\n",
      "[ CPUFloatType{2,2} ]\n",
      "MulBackward1\n",
      "27\n",
      "[ CPUFloatType{} ]\n",
      "MeanBackward0\n"
     ]
    }
   ],
   "source": [
    "auto z = y * y * 3;\n",
    "auto out = z.mean();\n",
    "\n",
    "std::cout << z << std::endl;\n",
    "std::cout << z.grad_fn()->name() << std::endl;\n",
    "std::cout << out << std::endl;\n",
    "std::cout << out.grad_fn()->name() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.requires_grad_( ... )` changes an existing tensor’s `requires_grad` flag in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  1\n",
      " 1  1\n",
      "[ CPUFloatType{2,2} ]\n",
      " 1.6368  0.9471\n",
      " 1.6281 -0.1512\n",
      "[ CPUFloatType{2,2} ]\n",
      " 2.6368  1.9471\n",
      " 2.6281  0.8488\n",
      "[ CPUFloatType{2,2} ]\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor a_tensor = torch::ones({2, 2}, torch::requires_grad());\n",
    "torch::Tensor b_tensor = torch::randn({2, 2});\n",
    "\n",
    "std::cout << a_tensor << std::endl;\n",
    "std::cout << b_tensor << std::endl;\n",
    "\n",
    "auto c_tensor = a_tensor + b_tensor;\n",
    "c_tensor.grad();\n",
    "\n",
    "std::cout << c_tensor << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "SumBackward0\n"
     ]
    }
   ],
   "source": [
    "auto a = torch::randn({2, 2});\n",
    "a = ((a * 3) / (a - 1));\n",
    "std::cout << a.requires_grad() << std::endl;\n",
    "\n",
    "a.requires_grad_(true);\n",
    "std::cout << a.requires_grad() << std::endl;\n",
    "\n",
    "auto b = (a * a).sum();\n",
    "std::cout << b.grad_fn()->name() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s backprop now. Because out contains a single scalar, `out.backward()`is equivalent to `out.backward(torch::tensor(1.))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients d(out)/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[ CPUFloatType{2,2} ]\n"
     ]
    }
   ],
   "source": [
    "std::cout << x.grad() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have got a matrix of `4.5`. For explanations on how we arrive at this value, please see [the corresponding section in this tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients).\n",
    "\n",
    "Now let’s take a look at an example of vector-Jacobian product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1094.5283\n",
      "  -95.9115\n",
      " -184.5223\n",
      "[ CPUFloatType{3} ]\n",
      "MulBackward1\n"
     ]
    }
   ],
   "source": [
    "x = torch::randn(3, torch::requires_grad());\n",
    "\n",
    "y = x * 2;\n",
    "while (y.norm().item<double>() < 1000) {\n",
    "  y = y * 2;\n",
    "}\n",
    "\n",
    "std::cout << y << std::endl;\n",
    "std::cout << y.grad_fn()->name() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the vector-Jacobian product, pass the vector to `backward` as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  102.4000\n",
      " 1024.0000\n",
      "    0.1024\n",
      "[ CPUFloatType{3} ]\n"
     ]
    }
   ],
   "source": [
    "auto v = torch::tensor({0.1, 1.0, 0.0001}, torch::kFloat);\n",
    "y.backward(v);\n",
    "\n",
    "std::cout << x.grad() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on tensors that require gradients either by putting `torch::NoGradGuard` in a code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "std::cout << x.requires_grad() << std::endl;\n",
    "std::cout << x.pow(2).requires_grad() << std::endl;\n",
    "\n",
    "{\n",
    "  torch::NoGradGuard no_grad;\n",
    "  std::cout << x.pow(2).requires_grad() << std::endl;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by using `.detach()` to get a new tensor with the same content but that does not require gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "std::cout << x.requires_grad() << std::endl;\n",
    "y = x.detach();\n",
    "std::cout << y.requires_grad() << std::endl;\n",
    "std::cout << x.eq(y).all().item<bool>() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on C++ tensor autograd APIs such as `grad` / `requires_grad `/ `is_leaf`/ `backward` / `detach` / `detach_` / `register_hook` / `retain_grad`, please see [the corresponding C++ API docs](https://pytorch.org/cppdocs/api/classat_1_1_tensor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing higher-order gradients in C++\n",
    "\n",
    "One of the applications of higher-order gradients is calculating gradient penalty. Let’s see an example of it using `torch::autograd::grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0379 -0.1198 -0.1874  0.2510\n",
      " 0.0507  0.0648  0.0742 -0.0860\n",
      "-0.0092  0.0444  0.0366 -0.0218\n",
      "[ CPUFloatType{3,4} ]\n"
     ]
    }
   ],
   "source": [
    "#include <torch/torch.h>\n",
    "\n",
    "auto model = torch::nn::Linear(4, 3);\n",
    "\n",
    "auto input = torch::randn({3, 4}).requires_grad_(true);\n",
    "auto output = model(input);\n",
    "\n",
    "// Calculate loss\n",
    "auto target = torch::randn({3, 3});\n",
    "auto loss = torch::nn::MSELoss()(output, target);\n",
    "\n",
    "// Use norm of gradients as penalty\n",
    "auto grad_output = torch::ones_like(output);\n",
    "auto gradient = torch::autograd::grad({output}, {input}, /*grad_outputs=*/{grad_output}, /*create_graph=*/true)[0];\n",
    "auto gradient_penalty = torch::pow((gradient.norm(2, /*dim=*/1) - 1), 2).mean();\n",
    "\n",
    "// Add gradient penalty to loss\n",
    "auto combined_loss = loss + gradient_penalty;\n",
    "combined_loss.backward();\n",
    "\n",
    "std::cout << input.grad() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the documentation for `torch::autograd::backward` ([link](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1afa9b5d4329085df4b6b3d4b4be48914b.html)) and `torch::autograd::grad` ([link](https://pytorch.org/cppdocs/api/function_namespacetorch_1_1autograd_1a1e03c42b14b40c306f9eb947ef842d9c.html)) for more information on how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom autograd function in C++\n",
    "\n",
    "(Adapted from this [tutorial](https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd))\n",
    "\n",
    "Adding a new elementary operation to torch::autograd requires implementing a new `torch::autograd::Function` subclass for each operation. `torch::autograd::Function` s are what `torch::autograd` uses to compute the results and gradients, and encode the operation history. Every new function requires you to implement 2 methods: `forward` and `backward`, and please see this [link](https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html) for the detailed requirements.\n",
    "\n",
    "Below you can find code for a `Linear` function from `torch::nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <torch/torch.h>\n",
    "\n",
    "using namespace torch::autograd;\n",
    "\n",
    "// Inherit from Function\n",
    "class LinearFunction : public Function<LinearFunction> {\n",
    " public:\n",
    "  // Note that both forward and backward are static functions\n",
    "\n",
    "  // bias is an optional argument\n",
    "  static torch::Tensor forward(\n",
    "      AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) {\n",
    "    ctx->save_for_backward({input, weight, bias});\n",
    "    auto output = input.mm(weight.t());\n",
    "    if (bias.defined()) {\n",
    "      output += bias.unsqueeze(0).expand_as(output);\n",
    "    }\n",
    "    return output;\n",
    "  }\n",
    "\n",
    "  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n",
    "    auto saved = ctx->get_saved_variables();\n",
    "    auto input = saved[0];\n",
    "    auto weight = saved[1];\n",
    "    auto bias = saved[2];\n",
    "\n",
    "    auto grad_output = grad_outputs[0];\n",
    "    auto grad_input = grad_output.mm(weight);\n",
    "    auto grad_weight = grad_output.t().mm(input);\n",
    "    auto grad_bias = torch::Tensor();\n",
    "    if (bias.defined()) {\n",
    "      grad_bias = grad_output.sum(0);\n",
    "    }\n",
    "\n",
    "    return {grad_input, grad_weight, grad_bias};\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the `LinearFunction` in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5197 -0.9382 -0.8346\n",
      "-1.5197 -0.9382 -0.8346\n",
      "[ CPUFloatType{2,3} ]\n",
      "-1.7077 -1.1958 -2.2832\n",
      "-1.7077 -1.1958 -2.2832\n",
      "-1.7077 -1.1958 -2.2832\n",
      "-1.7077 -1.1958 -2.2832\n",
      "[ CPUFloatType{4,3} ]\n"
     ]
    }
   ],
   "source": [
    "auto x = torch::randn({2, 3}).requires_grad_();\n",
    "auto weight = torch::randn({4, 3}).requires_grad_();\n",
    "auto y = LinearFunction::apply(x, weight);\n",
    "y.sum().backward();\n",
    "\n",
    "std::cout << x.grad() << std::endl;\n",
    "std::cout << weight.grad() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we give an additional example of a function that is parametrized by non-tensor arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <torch/torch.h>\n",
    "\n",
    "using namespace torch::autograd;\n",
    "\n",
    "class MulConstant : public Function<MulConstant> {\n",
    " public:\n",
    "  static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) {\n",
    "    // ctx is a context object that can be used to stash information\n",
    "    // for backward computation\n",
    "    ctx->saved_data[\"constant\"] = constant;\n",
    "    return tensor * constant;\n",
    "  }\n",
    "\n",
    "  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n",
    "    // We return as many input gradients as there were arguments.\n",
    "    // Gradients of non-tensor arguments to forward must be `torch::Tensor()`.\n",
    "    return {grad_outputs[0] * ctx->saved_data[\"constant\"].toDouble(), torch::Tensor()};\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use the `MulConstant` in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5.5000\n",
      " 5.5000\n",
      "[ CPUFloatType{2} ]\n"
     ]
    }
   ],
   "source": [
    "auto x = torch::randn({2}).requires_grad_();\n",
    "auto y = MulConstant::apply(x, 5.5);\n",
    "y.sum().backward();\n",
    "\n",
    "std::cout << x.grad() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on `torch::autograd::Function`, please see [its documentation](https://pytorch.org/cppdocs/api/structtorch_1_1autograd_1_1_function.html)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
